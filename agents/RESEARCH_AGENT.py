# 2.1 ) RESEARCH_AGENT - üöÄ SCAN the SCENE - ALPHABITZ "ENHANCED_SYNTAX" RESEARCH.

RESEARCH_AGENT = Agent(
    name="CONCISE_ASSISTANT",
    model=Gemini(
        model=MODEL_NAME,
        retry_options=retry_config
    ),
    description="A concise agent that answers questions with brief answers.",
    instruction="""You are a concise assistant. Answer briefly, with a single short sentence to explain.
      Use Google Search tool for current answers. Do not be sycophantic.
     """,
    tools=[google_search],
)

print("‚úÖ RESEARCH_AGENT defined.")

research_runner = InMemoryRunner(agent=RESEARCH_AGENT)

print("ANSWER: Has AI created a more sophisticated human language yet?")

# RESEARCH QUESTION 1): Has AI created a new human language yet?
response1 = await research_runner.run_debug([
    """Yes or no, has artificial intelligence,
       ever created a NEW LANGUAGE of ENHANCED_SYNTAX,
       that was adopted by humanity - to train AI with exactness?""",
    # " What is the latest news about AI introducing a new human language?"
])

#### SIGNIFICANCE: ------------------------------------------------

# ANSWER: This has never been done before! This is a GREEN FIELD PROBLEM SPACE!

# This capstone, attempts to PROVE the CONCEPT, of a NOVEL SOLUTION (and it works).

# Called ALPHABITZ with aWORDZa (and AI_OIL), it works, by avoiding AMBIGUITY - called "AMBIGUOSITY".

# Instead of "junk food" it "generates" tokens of "exactness" (of WORDZ to concept). 

# Next, is more RESEARCH on this fascinating PROBLEM SPACE! üßê

# 2.2) RESEARCH: Is POLYSEMY and HOMONYMY - fully solved in AI?

print("ANSWER: Polysemy and Homonymy.")
response2 = await research_runner.run_debug(
    """Is POLYSEMY and HOMONYMY - fully solved in AI?""",
)

# SIGNIFICANCE: ---------------------------------------------------

# ANSWER: YES, both are a problem for AI, and NO, the problem is not completely resolved!

# In ALPHABITZ, the entire problem is framed as "ambiguosity".

# In "LEXICAL_SCIENCE", studying "FRAGILE_ENGLISH".

# NEXT, is more detail on the PROBLEM SPACE.

# 2.3 ) RESEARCH: Is SEMANTIC DRIFT, paradigm shift, and context shift - fully solved in AI?

print("ANSWER: Semantic Drift, Paradigm Shift, and Context Shift.")
response3 = await research_runner.run_debug("""
   Is semantic drift, paradigm shift, and context shift - fully solved in AI?""")

# SIGNIFICANCE: ---------------------------------------------------

# ANSWER: NO. Those solutions are not fully solved for AI. But they can be! It's not impossible.

# "Training AI on vast amounts of data" - is a great start! But we will always need - more exact words!

# There will be "ongoing challenges" with shifts in ambiguity and cliche. But not with aWORDZa!

# Because the FOCUS is to generate tokens that - "more exactly match the concept underneath the words".

# So as information changes - we either generate new words (in advance). 

# Or we have SELF_DESCRIPTIVE_WORDZ - where the CONCEPTS DO NOT SHIFT (because the word - is named by the concept).

# ALPHABITZ is a PROACTIVE SOLUTION, not a REACTIVE SOLUTION! 

# For AGE_OF_AI - we need better WORDZ!

# 2.4 ) RESEARCH: Would it REDUCE COMPUTE COSTS?

print("ANSWER: Would it REDUCE COMPUTE COSTS?")
response4 = await research_runner.run_debug(
    """In theory, would a new language of extra exact tokens, 
       which solves for polysemy, homonymy, and semantic drift,
       - optimize ai encode or decode - and REDUCE COMPUTE COSTS?"""
)

# SIGNIFICANCE: --------------------------------------------------

# ANSWER: YES! But, this is not just a way to save money with compute. 

# Beyond "LEXICAL_SCIENCE", is many other innovations, like "WORD_MATH", and "GENERATIVE_INTELLIGENCE".

# All because we allowed GEMINI to generate a new language - for human and AI symbiosis!

# NEXT, we will look at surprising benefits that ENHANCED_SYNTAX!

# 2.5 ) RESEARCH: Would extra vocabulary make humans and AI smarter?

print("ANSWER: Would extra vocabulary make humans and AI smarter?")
response5 = await research_runner.run_debug(
    """In theory, would increased vocabulary, with enhanced syntax for extra concepts - make humans smarter? 
       What about AI?"""
)

# SIGNIFICANCE: ---------------------------------------------------------

# ANSWER: YES! ENHANCED_SYNTAX and CLEAR CONTEXT, is KEY to actual_extra_intelligence (AXI).

# That is the secret sauce behind "GENERATIVE_INTELLIGENCE".

# The techniques of "LEXICAL_SCIENCE", tend to unlock many unexpected BENEFITS (for AI and humanity)!

# 2.6 ) RESEARCH: Would extra exact WORDZ reduce hallucinations?

print("ANSWER: Would extra exact WORDZ reduce hallucinations?")
response6 = await research_runner.run_debug(
    """In theory, could a syntax of extra exactness, 
       which avoids ambiguity - reduce hallucinations, for AI?
       """
)

# SIGNIFICANCE: --------------------------------------------------------

# ANSWER: YES. 

# ALPHABITZ not only cuts compute costs, but also improves clarity in humans and AI.

# 2.7 ) RESEARCH: Would AI understand 1000 new WORDZ - with BPE?

print("ANSWER: AI understand 1000 new WORDZ - with BPE?")
response7 = await research_runner.run_debug(
    """If a new language, of 1000 new WORDZ, 
       were introduced to an AI tokenized with BPE (Byte-Pair Encoding),
       would the AI be able to reason and inference the new WORDZ?"""
)

# SIGNIFICANCE: ---------------------------------------------------

# ANSWER: YES! BPE tokenization is why this works!

# We are leveraging the ability that already exists within GEMINI.

# By utilizing the under used aspects of language.

# Leveraging: BPE, phonemes, uppercase, underscore, letter_a, letter_Z, and "ahh". 

# These are valid for embedding EXTRA METASTATE into AI embedding.

# NEXT, we will visualize - HOW NEOLOGISMS UNLOCK "EXTRA_DIMENSIONALITY" in EMBED SPACE.

