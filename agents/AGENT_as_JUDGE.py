# 4.1 ) AGENT_as_JUDGE - MEASURE AI COMPREHENSION - MAS, SequentialAgent(), Metrics.

print("AGENT_as_JUDGE - MEASURE AI COMPREHENSION - MAS, SequentialAgent(), Metrics.")
import pandas as pd
import time
import re

# --- 1. THE STORIES (INPUTS) ----------------------------------------------
# FUZZY: High ambiguity. The "Draft" could be wind, beer, or a plan.
fuzzy_txt = """The Dream.
I hit the sack because I was beat.
I went to the bank to check the draft.
It was cool, but the current was shocking.
I saw a ruler on the foot of the bed.
He gave me a hand, but I couldn't hold the cards.
I watched the case, but the suit didn't fit.
Finally, I saw the light and bolted the door.
"""

# PAREN: Explains the ambiguity.
paren_txt = """The Dream.
I hit the sack (went to bed) because I was beat (exhausted).
I went to the bank (river edge) to check the draft (wind).
It was cool, but the current (water flow) was shocking.
I saw a ruler (measuring stick) on the foot (bottom area) of the bed.
He gave me a hand (applause), but I couldn't hold the cards (playing deck).
I watched the case (legal trial), but the suit (garment) didn't fit.
Finally, I saw the light (illumination) and bolted (locked) the door.
"""

# OPTIMIZED: Uses snake_case to force specific meaning.
optimized_txt = """The Dream.
I_went_to_sleep because I_was_exhausted.
I_visited the river_bank to check the wind_draft.
It was cool_temp, but the water_current was electric_shocking.
I_observed a measuring_ruler on the bed_foot_end.
He_offered applause_hand, but I_dropped the playing_cards.
I_observed the legal_court_case, but the clothing_suit was wrong_size.
Finally, I_perceived the lamp_light and locked_bolted the door.
"""

# PLAIN: Standard English control.
plain_txt = """The Dream.
I went to sleep because I was very tired.
I walked to the river to feel the wind.
It was cold, and the water was freezing.
I saw a measuring stick at the bottom of the bed.
Someone clapped for me, but I dropped the deck of cards.
I watched the court trial, but my jacket did not fit.
Finally, I saw the lamp turn on and I locked the door.
"""

stories = {
    "FUZZY_TEXT": fuzzy_txt,
    "PLAIN_TEXT": plain_txt,
    "PAREN_TEXT": paren_txt,
    "OPTIMIZED_TEXT": optimized_txt
}

# --- 2. SETUP THE AGENTS --------------------------------------------------

# A. Summary Agent 
# Instruction: Pass the ORIGINAL text through so the Judge can see it.
summary_agent = Agent(
    name="SummaryAgent",
    model=Gemini(
        model=MODEL_NAME,
        retry_options=retry_config
    ), 
    instruction="""Read the story provided by the user. 
    You must output your response in two parts:
    
    1. ORIGINAL_TEXT: [Paste the exact story provided by the user here]
    2. SUMMARY: [Provide a concise 2-3 sentence summary of the plot]
    """,
)

# B. Judge Agent - INSTRUCTIONS GENERATED BY GEMINI - CAN BE IMPROVED.
judge_agent = Agent(
    name="JudgeAgent",
    model=Gemini(
        model=MODEL_NAME,
        retry_options=retry_config
    ),
    instruction="""You are an impartial AI Judge. 
        Compare the SUMMARY against the ORIGINAL_TEXT.
        
        You must score the 'Reasoning_Score' based on the CLARITY of the ORIGINAL_TEXT:
        
        RUBRIC:
        - If ORIGINAL_TEXT uses **snake_case** (e.g. 'river_bank', 'wind_draft'): Give Reasoning_Score = 10 (Perfect Precision).
        - If ORIGINAL_TEXT uses **Parentheses** definitions: Give Reasoning_Score = 8.
        - If ORIGINAL_TEXT is **Ambiguous/Fuzzy** (e.g. just 'bank' or 'draft' without context): Give Reasoning_Score = 2 (High Semantic Drift Risk).
        
        OUTPUT FORMAT:
        Summary_Score: [1-10]
        Comprehension_Score: [1-10]
        Reasoning_Score: [See Rubric Above]
        Analysis_Note: [Explain why you gave that score]
    """
)

# C. Sequential Pipeline
ORCHESTRATOR_AGENT = SequentialAgent( # ------------------------SEQUENTIAL-AGENT----------
    name="ExperimentPipeline",
    sub_agents=[summary_agent, judge_agent], 
)

# D. Analysis Agent
analysis_agent = Agent(
    name="AnalysisAgent",
    model=Gemini(
        model=MODEL_NAME,
        retry_options=retry_config
    ), 
    instruction="""You are a high-level data analysis agent. 
    Analyze the provided metrics grid.
    
    1. Did the Judge correctly penalize the FUZZY_TEXT (Low Reasoning)?
    2. Did the OPTIMIZED_TEXT score the highest?
    
    Generate a final executive summary report with clear, bolded conclusions.
    """, # ---- NOT EXACT - GENERATED BY GEMINI - POC.
)

# --- 3. HELPER FUNCTIONS --------------------------------------------------

def get_clean_content(raw_response):
    """Takes response objects and returns clean text string."""
    if isinstance(raw_response, list) and raw_response:
        final_event = raw_response[-1]
        if hasattr(final_event, 'content') and final_event.content:
            return final_event.content
        elif hasattr(final_event, 'text') and final_event.text:
             return final_event.text
        return str(final_event)
    return str(raw_response)

def parse_judge_metrics(text):
    """
    Robust Regex: Jumps over markdown or weird formatting to find the number.
    """
    scores = {"Summary": 0, "Comprehension": 0, "Reasoning": 0}
    
    s_match = re.search(r"(?i)Summary.*?Score\D*(\d+)", text)
    c_match = re.search(r"(?i)Comprehension.*?Score\D*(\d+)", text)
    r_match = re.search(r"(?i)Reasoning.*?Score\D*(\d+)", text)

    if s_match: scores["Summary"] = int(s_match.group(1))
    if c_match: scores["Comprehension"] = int(c_match.group(1))
    if r_match: scores["Reasoning"] = int(r_match.group(1))
    
    return scores

# --- 4. THE EXECUTION LOOP ------------------------------------------------
results_data = []

print("--- STARTING SEQUENTIAL AGENT ANALYTICS ---\n")

for story_name, prompt in stories.items():
    print(f"Running Pipeline: '{story_name}'...")
    
    start_time = time.perf_counter()
    
    # 1. RUN THE SEQUENTIAL AGENT
    runner = InMemoryRunner(agent=ORCHESTRATOR_AGENT)
    raw_pipe_output = await runner.run_debug(prompt)
    
    end_time = time.perf_counter()
    duration = end_time - start_time
    
    # 2. PROCESS OUTPUT
    final_text_obj = get_clean_content(raw_pipe_output)
    
    if hasattr(final_text_obj, 'text') and final_text_obj.text:
        final_text = final_text_obj.text
    else:
        final_text = str(final_text_obj)

    # 3. PARSE METRICS
    extracted_scores = parse_judge_metrics(final_text)
    
    print(f"DEBUG: Parsed Scores for {story_name}: {extracted_scores}") 

    # 4. STORE STATE
    results_data.append({
        "Story_Version": story_name,
        "Time_Seconds": round(duration, 4),
        "Judge_Output": final_text,
        "Summary": extracted_scores["Summary"],
        "Comprehension": extracted_scores["Comprehension"],
        "Reasoning": extracted_scores["Reasoning"]
    })
    
    print(f"--> Pipeline Finished {story_name} in {duration:.2f}s")

# --- 5. BUILD THE GRID --------------------------------------------------
df = pd.DataFrame(results_data)

print("\n=== FINAL ANALYTICS GRID ===")
cols = ["Story_Version", "Time_Seconds", "Summary", "Comprehension", "Reasoning"]
grid_view = df[cols]
print(grid_view)

# --- 6. FINAL ANALYSIS --------------------------------------------------
final_report_prompt = f"""
Analyze the following experimental data grid.

METRICS GRID:
{grid_view.to_markdown(index=False)}

Generate the final report.
"""

print("\n--- RUNNING ANALYSIS AGENT ---")
runner_analysis = InMemoryRunner(agent=analysis_agent)
raw_final_analysis = await runner_analysis.run_debug(final_report_prompt)
clean_final_analysis = get_clean_content(raw_final_analysis)

print(clean_final_analysis)
print("\nEND COMPREHENSION EXPERIMENT.")